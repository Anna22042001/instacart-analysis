{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport copy\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-15T13:47:08.443685Z","iopub.execute_input":"2022-06-15T13:47:08.446223Z","iopub.status.idle":"2022-06-15T13:47:15.324040Z","shell.execute_reply.started":"2022-06-15T13:47:08.444254Z","shell.execute_reply":"2022-06-15T13:47:15.322830Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class DataFrame(object):\n\n    \"\"\"Minimal pd.DataFrame analog for handling n-dimensional numpy matrices with additional\n    support for shuffling, batching, and train/test splitting.\n\n    Args:\n        columns: List of names corresponding to the matrices in data.\n        data: List of n-dimensional data matrices ordered in correspondence with columns.\n            All matrices must have the same leading dimension.  Data can also be fed a list of\n            instances of np.memmap, in which case RAM usage can be limited to the size of a\n            single batch.\n    \"\"\"\n\n    def __init__(self, columns, data):\n        assert len(columns) == len(data), 'columns length does not match data length'\n\n        lengths = [mat.shape[0] for mat in data]\n        assert len(set(lengths)) == 1, 'all matrices in data must have same first dimension'\n\n        self.length = lengths[0]\n        self.columns = columns\n        self.data = data\n        self.dict = dict(zip(self.columns, self.data))\n        self.idx = np.arange(self.length)\n\n    def shapes(self):\n        return pd.Series(dict(zip(self.columns, [mat.shape for mat in self.data])))\n\n    def dtypes(self):\n        return pd.Series(dict(zip(self.columns, [mat.dtype for mat in self.data])))\n\n    def shuffle(self):\n        np.random.shuffle(self.idx)\n\n    def train_test_split(self, train_size, random_state=np.random.randint(10000)):\n        train_idx, test_idx = train_test_split(self.idx, train_size=train_size, random_state=random_state)\n        train_df = DataFrame(copy.copy(self.columns), [mat[train_idx] for mat in self.data])\n        test_df = DataFrame(copy.copy(self.columns), [mat[test_idx] for mat in self.data])\n        return train_df, test_df\n\n    def batch_generator(self, batch_size, shuffle=True, num_epochs=3, allow_smaller_final_batch=False):\n        epoch_num = 0\n        while epoch_num < num_epochs:\n            if shuffle:\n                self.shuffle()\n\n            for i in range(0, self.length, batch_size):\n                batch_idx = self.idx[i: i + batch_size]\n                if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n                    break\n                yield DataFrame(columns=copy.copy(self.columns), data=[mat[batch_idx].copy() for mat in self.data])\n\n            epoch_num += 1\n\n    def iterrows(self):\n        for i in self.idx:\n            yield self[i]\n\n    def mask(self, mask):\n        return DataFrame(copy.copy(self.columns), [mat[mask] for mat in self.data])\n\n    def __iter__(self):\n        return self.dict.items().__iter__()\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, key):\n        if isinstance(key, str):\n            return self.dict[key]\n\n        elif isinstance(key, int):\n            return pd.Series(dict(zip(self.columns, [mat[self.idx[key]] for mat in self.data])))\n\n    def __setitem__(self, key, value):\n        assert value.shape[0] == len(self), 'matrix first dimension does not match'\n        if key not in self.columns:\n            self.columns.append(key)\n            self.data.append(value)\n        self.dict[key] = value","metadata":{"execution":{"iopub.status.busy":"2022-06-15T13:47:15.325857Z","iopub.execute_input":"2022-06-15T13:47:15.326553Z","iopub.status.idle":"2022-06-15T13:47:15.351694Z","shell.execute_reply.started":"2022-06-15T13:47:15.326518Z","shell.execute_reply":"2022-06-15T13:47:15.350447Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class DataReader(object):\n\n    def __init__(self, data_dir):\n        data_cols = ['x', 'y']\n        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n\n        df = DataFrame(columns=data_cols, data=data)\n\n        self.train_df, self.val_df = df.train_test_split(train_size=0.9)\n\n        self.num_products = df['x'].max() + 1\n        self.product_dist = np.bincount(self.train_df['x']).tolist()\n\n    def train_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.train_df,\n            shuffle=True,\n            num_epochs=50\n        )\n\n    def val_batch_generator(self, batch_size):\n        return self.batch_generator(\n            batch_size=batch_size,\n            df=self.val_df,\n            shuffle=True,\n            num_epochs=50\n        )\n\n    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n        return df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-15T13:47:15.353132Z","iopub.execute_input":"2022-06-15T13:47:15.353517Z","iopub.status.idle":"2022-06-15T13:47:15.366797Z","shell.execute_reply.started":"2022-06-15T13:47:15.353463Z","shell.execute_reply":"2022-06-15T13:47:15.366055Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def sequence_log_loss(y, y_hat, sequence_lengths, max_sequence_length, eps=1e-15):\n    y = tf.cast(y, tf.float32)\n    y_hat = tf.math.minimum(tf.math.maximum(y_hat, eps), 1.0 - eps)\n    log_losses = y*tf.math.log(y_hat) + (1.0 - y)*tf.math.log(1.0 - y_hat)\n    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n    avg_log_loss = -tf.reduce_sum(log_losses*sequence_mask) / tf.cast(tf.math.reduce_sum(sequence_lengths), tf.float32)\n    return avg_log_loss\ndef log_loss(y, y_hat, eps=1e-15):\n    y = tf.cast(y, tf.float32)\n    y_hat = tf.math.minimum(tf.math.maximum(y_hat, eps), 1.0 - eps)\n    log_loss = -tf.math.reduce_mean(y*tf.math.log(y_hat) + (1.0 - y)*tf.math.log(1.0 - y_hat))\n    return log_loss","metadata":{"execution":{"iopub.status.busy":"2022-06-15T13:47:15.368913Z","iopub.execute_input":"2022-06-15T13:47:15.369613Z","iopub.status.idle":"2022-06-15T13:47:15.384329Z","shell.execute_reply.started":"2022-06-15T13:47:15.369564Z","shell.execute_reply":"2022-06-15T13:47:15.383371Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class SGNS(tf.keras.Model):\n    def __init__(self,embedding_dim, negative_samples, num_products, product_dist):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.negative_samples = negative_samples\n        self.num_products = num_products\n        self.product_dist = product_dist\n        self.x = tf.keras.Input(shape=[], dtype = tf.int32)\n        self.y = tf.keras.Input(shape=[], dtype = tf.int32)\n        self.embeddings = tf.Variable(\n            tf.random.uniform([num_products, embedding_dim], -1.0, 1.0))\n        self.nce_weights = tf.Variable(\n            tf.random.truncated_normal(\n                shape=[num_products, embedding_dim],\n                stddev=1.0 / np.sqrt(embedding_dim)\n            )\n        )\n        self.nce_biases = tf.Variable(tf.zeros([num_products]))\n    def call(self,inputs):\n        for placeholder_name, data in inputs:\n            if hasattr(self, placeholder_name):\n                setattr(self, placeholder_name, np.asarray(data, dtype=np.int32))\n        inputs = tf.nn.embedding_lookup(self.embeddings, self.x)\n        sampled_values = tf.nn.fixed_unigram_candidate_sampler(\n            true_classes=tf.cast(tf.reshape(self.y, (-1, 1)), tf.int64),\n            num_true=1,\n            num_sampled=self.negative_samples,\n            unique=True,\n            range_max=self.num_products,\n            distortion=0.75,\n            unigrams=self.product_dist\n        )\n        loss = tf.reduce_mean(\n            tf.nn.nce_loss(\n                weights=self.nce_weights,\n                biases=self.nce_biases,\n                labels=self.y,\n                inputs=inputs,\n                num_sampled=self.negative_samples,\n                num_classes=self.num_products,\n                sampled_values=sampled_values\n            )\n        )\n\n        self.parameter_tensors = {\n            'product_embeddings': self.embeddings\n        }\n        return loss\n","metadata":{"execution":{"iopub.status.busy":"2022-06-15T13:47:15.385829Z","iopub.execute_input":"2022-06-15T13:47:15.386461Z","iopub.status.idle":"2022-06-15T13:47:16.902230Z","shell.execute_reply.started":"2022-06-15T13:47:15.386417Z","shell.execute_reply":"2022-06-15T13:47:16.901147Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\nbase_dir = '../input/'\ndr = DataReader(data_dir=os.path.join(base_dir, 'instacartsgns'))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T13:47:16.903595Z","iopub.execute_input":"2022-06-15T13:47:16.904757Z","iopub.status.idle":"2022-06-15T13:47:18.049553Z","shell.execute_reply.started":"2022-06-15T13:47:16.904708Z","shell.execute_reply":"2022-06-15T13:47:18.048683Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"sgns = SGNS(25,100,num_products = dr.num_products, product_dist=dr.product_dist)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T13:47:18.050699Z","iopub.execute_input":"2022-06-15T13:47:18.051002Z","iopub.status.idle":"2022-06-15T13:47:18.598116Z","shell.execute_reply.started":"2022-06-15T13:47:18.050956Z","shell.execute_reply":"2022-06-15T13:47:18.596824Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = dr.train_batch_generator(128)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T13:47:18.599528Z","iopub.execute_input":"2022-06-15T13:47:18.599862Z","iopub.status.idle":"2022-06-15T13:47:18.604157Z","shell.execute_reply.started":"2022-06-15T13:47:18.599831Z","shell.execute_reply":"2022-06-15T13:47:18.603296Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import time\nepochs = 50\noptimizer = tf.keras.optimizers.Adam(0.002)\nstep0 = 0\nfor epoch in range(epochs):\n    print(\"\\nStart of epoch %d\" % (epoch,))\n    start_time = time.time()\n\n    # Iterate over the batches of the dataset.\n    train = list()\n    for step, x_batch_train in enumerate(train_dataset):\n        step0 += 1\n        with tf.GradientTape() as tape:\n            loss_value = sgns(x_batch_train, training = True)\n            train.append(loss_value)\n        grads = tape.gradient(loss_value, sgns.trainable_weights)\n        optimizer.apply_gradients(zip(grads, sgns.trainable_weights))\n        # Log every 200 batches.\n        if step % 200 == 0:\n            print(\n                \"Training loss (for one batch) at step %d: %.4f\"\n                % (step, float(loss_value))\n            )\n            print(\"Seen so far: %d samples\" % ((step + 1) * 128))\n        if step % 2728 == 0 and step > 0:\n            print(sum(train)/len(train))\n            break\n        if step0 > 60000:\n            break\n    if step0 > 60000:\n        break\n    val = list()\n    for step1, x_batch_val in enumerate(val_dataset):\n        loss_value = sgns(x_batch_val, training=True)\n        val.append(loss_value)\n        if step1 % 10 == 0:\n            print(\"validation\", step1)\n            print(loss_value)\n        if step1 % 303 == 0 and step1 > 0:\n            print(sum(val)/len(val))\n            break\n            ","metadata":{"execution":{"iopub.status.busy":"2022-06-15T13:47:54.265598Z","iopub.execute_input":"2022-06-15T13:47:54.265995Z","iopub.status.idle":"2022-06-15T13:49:10.543425Z","shell.execute_reply.started":"2022-06-15T13:47:54.265941Z","shell.execute_reply":"2022-06-15T13:49:10.541497Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}