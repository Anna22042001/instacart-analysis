{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:24:40.563232Z",
     "iopub.status.busy": "2022-06-15T13:24:40.562787Z",
     "iopub.status.idle": "2022-06-15T13:24:40.570010Z",
     "shell.execute_reply": "2022-06-15T13:24:40.568755Z",
     "shell.execute_reply.started": "2022-06-15T13:24:40.563199Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model (LSTM, Wavenet, Dense)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:24:42.194674Z",
     "iopub.status.busy": "2022-06-15T13:24:42.194246Z",
     "iopub.status.idle": "2022-06-15T13:24:42.245292Z",
     "shell.execute_reply": "2022-06-15T13:24:42.243981Z",
     "shell.execute_reply.started": "2022-06-15T13:24:42.194642Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, state_size, return_final_state=False):\n",
    "        super().__init__()\n",
    "        self.cell_fw = tf.keras.layers.LSTMCell(state_size)\n",
    "        self.return_final_state = return_final_state\n",
    "        self.rnn_layers = tf.keras.layers.RNN(cell = [tf.keras.layers.LSTMCell(state_size)], return_sequences=True, return_state=True)\n",
    "    def call(self, inputs):\n",
    "        outputs, output_state = self.rnn_layers(inputs)\n",
    "        if self.return_final_state:\n",
    "            return outputs, output_state\n",
    "        else:\n",
    "            return outputs\n",
    "class TCL(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_units, convolution_width, causal=False, dilation_rate=[32], bias=True, activation=None, dropout=None):\n",
    "        super().__init__()\n",
    "        self.output_units = output_units\n",
    "        self.convolution_width = convolution_width\n",
    "        self.causal = causal\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "    def build(self, inputs_shape):\n",
    "        self.w = tf.Variable(name='weights', initial_value=tf.keras.initializers.VarianceScaling()(shape=[self.convolution_width, inputs_shape[2], self.output_units]), dtype=tf.float32)\n",
    "        if self.bias:\n",
    "            self.b = tf.Variable(name='biases', initial_value=tf.constant_initializer(0.1)(shape=[self.output_units]), dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        if self.causal:\n",
    "            shift = self.dilation_rate[0]*(self.convolution_width-1)\n",
    "            pad = tf.zeros(shape=[tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n",
    "            inputs = tf.concat([pad, inputs], axis=1)\n",
    "        z = tf.nn.convolution(inputs, self.w, padding='VALID', dilations=self.dilation_rate)\n",
    "        if self.bias:\n",
    "            z = z+self.b\n",
    "        z = self.activation(z) if self.activation else z\n",
    "        dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
    "        z = dropout_layer(z) if self.dropout is not None else z\n",
    "        return z\n",
    "class Time_distributed_dense_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self,output_units, bias=True, activation=None, batch_norm=None, dropout=None):\n",
    "        super().__init__()\n",
    "        self.output_units = output_units\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "    def build(self, inputs_shape):\n",
    "        self.w = tf.Variable(name='weights', initial_value=tf.keras.initializers.VarianceScaling()(shape=[inputs_shape[-1], self.output_units]), dtype=tf.float32)\n",
    "        if self.bias:\n",
    "            self.b = tf.Variable(name='biases', initial_value=tf.constant_initializer(0.1)(shape=[self.output_units]), dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        z = tf.einsum('ijk,kl->ijl', inputs, self.w)\n",
    "        if self.bias:\n",
    "            z = z + self.b\n",
    "        if self.batch_norm is not None:\n",
    "            batch_layer = tf.keras.layers.BatchNormalization()\n",
    "            z = batch_layer(z, training=self.batch_norm)\n",
    "        z = self.activation(z) if self.activation else z\n",
    "        dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
    "        z = dropout_layer(z) if self.dropout is not None else z\n",
    "        return z\n",
    "\n",
    "class Dense_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_units, bias=True, activation=None, batch_norm=None, dropout=None):\n",
    "        super().__init__()\n",
    "        self.output_units = output_units\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "    def build(self, inputs_shape):\n",
    "        self.w = tf.Variable(name='weights', initial_value=tf.keras.initializers.VarianceScaling()(shape=[inputs_shape[-1], self.output_units]), dtype=tf.float32)\n",
    "        if self.bias:\n",
    "            self.b = tf.Variable(name='biases', initial_value=tf.constant_initializer(0.05)(shape=[self.output_units]), dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.w)\n",
    "        if self.bias:\n",
    "            z = z+self.b\n",
    "        if self.batch_norm is not None:\n",
    "            batch_layer = tf.keras.layers.BatchNormalization()\n",
    "            z = batch_layer(z, training=self.batch_norm)\n",
    "        z = self.activation(z) if self.activation else z\n",
    "        dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
    "        z = dropout_layer(z) if self.dropout is not None else z\n",
    "        return z\n",
    "\n",
    "class Wavenet(tf.keras.layers.Layer):\n",
    "    def __init__(self, dilations, filter_widths, skip_channels, residual_channels):\n",
    "        super().__init__()\n",
    "        self.dilations = dilations\n",
    "        self.filter_widths = filter_widths\n",
    "        self.skip_channels = skip_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.tddl0 = Time_distributed_dense_layer(output_units = residual_channels, activation=tf.keras.activations.tanh)\n",
    "        self.tcl1 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[0]])\n",
    "        self.tcl2 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[1]])\n",
    "        self.tcl3 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[2]])\n",
    "        self.tcl4 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[3]])\n",
    "        self.tcl5 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[4]])\n",
    "        self.tcl6 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[5]])\n",
    "        self.list_tcl = list()\n",
    "        self.list_tcl.append(self.tcl1)\n",
    "        self.list_tcl.append(self.tcl2)\n",
    "        self.list_tcl.append(self.tcl3)\n",
    "        self.list_tcl.append(self.tcl4)\n",
    "        self.list_tcl.append(self.tcl5)\n",
    "        self.list_tcl.append(self.tcl6)\n",
    "        self.tddl1 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.tddl2 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.tddl3 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.tddl4 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.tddl5 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.tddl6 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.list_tddl = list()\n",
    "        self.list_tddl.append(self.tddl1)\n",
    "        self.list_tddl.append(self.tddl2)\n",
    "        self.list_tddl.append(self.tddl3)\n",
    "        self.list_tddl.append(self.tddl4)\n",
    "        self.list_tddl.append(self.tddl5)\n",
    "        self.list_tddl.append(self.tddl6)\n",
    "    def call(self, inputs):\n",
    "        z = self.tddl0(inputs)\n",
    "        skip_outputs = []\n",
    "        for i in range(6):\n",
    "            dilated_conv = self.list_tcl[i](z)\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.keras.activations.tanh(conv_filter)*tf.keras.activations.sigmoid(conv_gate)\n",
    "            output_units = self.skip_channels + self.residual_channels\n",
    "            outputs = self.list_tddl[i](dilated_conv)\n",
    "            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "            z += residuals\n",
    "            skip_outputs.append(skips)\n",
    "        skip_outputs = tf.keras.activations.relu(tf.concat(skip_outputs, axis=2))\n",
    "        return skip_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:24:46.719311Z",
     "iopub.status.busy": "2022-06-15T13:24:46.718884Z",
     "iopub.status.idle": "2022-06-15T13:24:46.761015Z",
     "shell.execute_reply": "2022-06-15T13:24:46.760030Z",
     "shell.execute_reply.started": "2022-06-15T13:24:46.719278Z"
    }
   },
   "outputs": [],
   "source": [
    "class rnn_product(tf.keras.Model):\n",
    "    def __init__(self, lstm_size, dilations, filter_widths, skip_channels, residual_channels):\n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.dilations = dilations\n",
    "        self.filter_widths = filter_widths\n",
    "        self.skip_channels = skip_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.user_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.product_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.aisle_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.department_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.is_none = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.history_length = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.is_ordered_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.index_in_order_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.order_dow_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.order_hour_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.days_since_prior_order_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.order_size_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.reorder_size_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.order_number_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.product_name = tf.keras.Input(shape=[30], dtype = tf.int32)\n",
    "        self.product_name_length = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.next_is_ordered = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.keep_prob = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.is_training = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.lstm = LSTM_layer(lstm_size)\n",
    "        self.wavenet = Wavenet(dilations, filter_widths, skip_channels, residual_channels)\n",
    "        self.tddl1 = Time_distributed_dense_layer(50, activation=tf.keras.activations.relu)\n",
    "        self.tddl2 = Time_distributed_dense_layer(1, activation=tf.keras.activations.sigmoid)\n",
    "        self.product_embeddings = tf.Variable(name='product_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[50000, self.lstm_size]),dtype=tf.float32)\n",
    "        \n",
    "        self.aisle_embeddings = tf.Variable(name='aisle_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[250, 50]), dtype=tf.float32)\n",
    "        self.department_embeddings = tf.Variable(name='department_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[50, 10]), dtype=tf.float32)\n",
    "        self.user_embeddings = tf.Variable(name='user_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[207000, self.lstm_size]), dtype=tf.float32)\n",
    "        self.dense = Dense_layer(100, activation=tf.keras.activations.relu)\n",
    "    def get_sequence(self):\n",
    "        product_names = tf.one_hot(self.product_name, 2532)\n",
    "        product_names = tf.math.reduce_max(product_names, 1)\n",
    "        product_names = self.dense(product_names)\n",
    "        is_none = tf.cast(tf.expand_dims(self.is_none, 1), tf.float32)\n",
    "        x_product = tf.concat([\n",
    "                               tf.nn.embedding_lookup(self.product_embeddings, self.product_id),\n",
    "                               tf.nn.embedding_lookup(self.aisle_embeddings, self.aisle_id),\n",
    "                               tf.nn.embedding_lookup(self.department_embeddings, self.department_id),\n",
    "                               is_none,\n",
    "                               product_names\n",
    "                               ], axis=1)\n",
    "        x_product = tf.tile(tf.expand_dims(x_product, 1), (1, 100, 1))\n",
    "        x_user = tf.nn.embedding_lookup(self.user_embeddings, self.user_id)\n",
    "        x_user = tf.tile(tf.expand_dims(x_user, 1), (1, 100, 1))\n",
    "        is_ordered_history = tf.one_hot(self.is_ordered_history, 2)\n",
    "        index_in_order_history = tf.one_hot(self.index_in_order_history, 20)\n",
    "        order_dow_history = tf.one_hot(self.order_dow_history, 8)\n",
    "        order_hour_history = tf.one_hot(self.order_hour_history, 25)\n",
    "        days_since_prior_order_history = tf.one_hot(self.days_since_prior_order_history, 31)\n",
    "        order_size_history = tf.one_hot(self.order_size_history, 60)\n",
    "        reorder_size_history = tf.one_hot(self.reorder_size_history, 50)\n",
    "        order_number_history = tf.one_hot(self.order_number_history, 101)\n",
    "        index_in_order_history_scalar = tf.expand_dims(tf.cast(self.index_in_order_history, tf.float32) / 20.0, 2)\n",
    "        order_dow_history_scalar = tf.expand_dims(tf.cast(self.order_dow_history, tf.float32) / 8.0, 2)\n",
    "        order_hour_history_scalar = tf.expand_dims(tf.cast(self.order_hour_history, tf.float32) / 25.0, 2)\n",
    "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(self.days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "        order_size_history_scalar = tf.expand_dims(tf.cast(self.order_size_history, tf.float32) / 60.0, 2)\n",
    "        reorder_size_history_scalar = tf.expand_dims(tf.cast(self.reorder_size_history, tf.float32) / 50.0, 2)\n",
    "        order_number_history_scalar = tf.expand_dims(tf.cast(self.order_number_history, tf.float32) / 100.0, 2)\n",
    "        x_history = tf.concat([\n",
    "                               is_ordered_history,\n",
    "                               index_in_order_history,\n",
    "                               order_dow_history,\n",
    "                               order_hour_history,\n",
    "                               days_since_prior_order_history,\n",
    "                               order_size_history,\n",
    "                               reorder_size_history,\n",
    "                               order_number_history,\n",
    "                               index_in_order_history_scalar,\n",
    "                               order_dow_history_scalar,\n",
    "                               order_hour_history_scalar,\n",
    "                               days_since_prior_order_history_scalar,\n",
    "                               order_size_history_scalar,\n",
    "                               reorder_size_history_scalar,\n",
    "                               order_number_history_scalar,\n",
    "                               ], axis=2)\n",
    "        x = tf.concat([x_history, x_product, x_user], axis=2)\n",
    "        return x    \n",
    "    def call(self, inputs):\n",
    "        for placeholder_name, data in inputs:\n",
    "            if hasattr(self, placeholder_name):\n",
    "                setattr(self, placeholder_name, np.asarray(data, dtype=np.int32))\n",
    "        x = self.get_sequence()\n",
    "        h = self.lstm(x)\n",
    "        c = self.wavenet(x)\n",
    "        h = tf.concat([h, c, x], axis=2)\n",
    "        self.h_final = self.tddl1(h)\n",
    "        y_hat = self.tddl2(self.h_final)\n",
    "        y_hat = tf.squeeze(y_hat, 2)\n",
    "        final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), self.history_length - 1], axis=1)\n",
    "        self.final_states = tf.gather_nd(self.h_final, final_temporal_idx)\n",
    "        self.final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
    "        self.prediction_tensors = {\n",
    "            'user_ids': self.user_id,\n",
    "            'product_ids': self.product_id,\n",
    "            'final_states': self.final_states,\n",
    "            'predictions': self.final_predictions\n",
    "            }\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:23:30.333827Z",
     "iopub.status.busy": "2022-06-14T07:23:30.333281Z",
     "iopub.status.idle": "2022-06-14T07:23:30.388876Z",
     "shell.execute_reply": "2022-06-14T07:23:30.38785Z",
     "shell.execute_reply.started": "2022-06-14T07:23:30.333786Z"
    }
   },
   "outputs": [],
   "source": [
    "# class rnn_product_bmm(tf.keras.Model):\n",
    "#     def __init__(self, lstm_size):\n",
    "#         super().__init__()\n",
    "#         self.lstm_size = lstm_size\n",
    "#         self.user_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "#         self.product_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "#         self.aisle_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "#         self.department_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "#         self.is_none = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "#         self.history_length = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "#         self.is_ordered_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "#         self.index_in_order_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "#         self.order_dow_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "#         self.order_hour_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "#         self.days_since_prior_order_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "#         self.order_size_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "#         self.reorder_size_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "#         self.order_number_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "#         self.product_name = tf.keras.Input(shape=[30], dtype = tf.int32)\n",
    "#         self.product_name_length = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "#         self.next_is_ordered = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "#         self.keep_prob = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "#         self.is_training = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "#         self.lstm = LSTM_layer(lstm_size)\n",
    "#         self.tddl1 = Time_distributed_dense_layer(50, activation=tf.keras.activations.relu)\n",
    "#         self.tddl2 = Time_distributed_dense_layer(2, activation=tf.keras.activations.sigmoid)\n",
    "#         self.product_embeddings = tf.Variable(name='product_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[50000, self.lstm_size]),dtype=tf.float32)\n",
    "        \n",
    "#         self.aisle_embeddings = tf.Variable(name='aisle_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[250, 50]), dtype=tf.float32)\n",
    "#         self.department_embeddings = tf.Variable(name='department_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[50, 10]), dtype=tf.float32)\n",
    "#         self.user_embeddings = tf.Variable(name='user_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[207000, self.lstm_size]), dtype=tf.float32)\n",
    "#         self.dense = Dense_layer(100, activation=tf.keras.activations.relu)\n",
    "#     def get_sequence(self):\n",
    "#         product_names = tf.one_hot(self.product_name, 2532)\n",
    "#         product_names = tf.math.reduce_max(product_names, 1)\n",
    "#         product_names = self.dense(product_names)\n",
    "#         is_none = tf.cast(tf.expand_dims(self.is_none, 1), tf.float32)\n",
    "#         x_product = tf.concat([\n",
    "#                                tf.nn.embedding_lookup(self.product_embeddings, self.product_id),\n",
    "#                                tf.nn.embedding_lookup(self.aisle_embeddings, self.aisle_id),\n",
    "#                                tf.nn.embedding_lookup(self.department_embeddings, self.department_id),\n",
    "#                                is_none,\n",
    "#                                product_names\n",
    "#                                ], axis=1)\n",
    "#         x_product = tf.tile(tf.expand_dims(x_product, 1), (1, 100, 1))\n",
    "#         x_user = tf.nn.embedding_lookup(self.user_embeddings, self.user_id)\n",
    "#         x_user = tf.tile(tf.expand_dims(x_user, 1), (1, 100, 1))\n",
    "#         is_ordered_history = tf.one_hot(self.is_ordered_history, 2)\n",
    "#         index_in_order_history = tf.one_hot(self.index_in_order_history, 20)\n",
    "#         order_dow_history = tf.one_hot(self.order_dow_history, 8)\n",
    "#         order_hour_history = tf.one_hot(self.order_hour_history, 25)\n",
    "#         days_since_prior_order_history = tf.one_hot(self.days_since_prior_order_history, 31)\n",
    "#         order_size_history = tf.one_hot(self.order_size_history, 60)\n",
    "#         reorder_size_history = tf.one_hot(self.reorder_size_history, 50)\n",
    "#         order_number_history = tf.one_hot(self.order_number_history, 101)\n",
    "#         index_in_order_history_scalar = tf.expand_dims(tf.cast(self.index_in_order_history, tf.float32) / 20.0, 2)\n",
    "#         order_dow_history_scalar = tf.expand_dims(tf.cast(self.order_dow_history, tf.float32) / 8.0, 2)\n",
    "#         order_hour_history_scalar = tf.expand_dims(tf.cast(self.order_hour_history, tf.float32) / 25.0, 2)\n",
    "#         days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(self.days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "#         order_size_history_scalar = tf.expand_dims(tf.cast(self.order_size_history, tf.float32) / 60.0, 2)\n",
    "#         reorder_size_history_scalar = tf.expand_dims(tf.cast(self.reorder_size_history, tf.float32) / 50.0, 2)\n",
    "#         order_number_history_scalar = tf.expand_dims(tf.cast(self.order_number_history, tf.float32) / 100.0, 2)\n",
    "#         x_history = tf.concat([\n",
    "#                                is_ordered_history,\n",
    "#                                index_in_order_history,\n",
    "#                                order_dow_history,\n",
    "#                                order_hour_history,\n",
    "#                                days_since_prior_order_history,\n",
    "#                                order_size_history,\n",
    "#                                reorder_size_history,\n",
    "#                                order_number_history,\n",
    "#                                index_in_order_history_scalar,\n",
    "#                                order_dow_history_scalar,\n",
    "#                                order_hour_history_scalar,\n",
    "#                                days_since_prior_order_history_scalar,\n",
    "#                                order_size_history_scalar,\n",
    "#                                reorder_size_history_scalar,\n",
    "#                                order_number_history_scalar,\n",
    "#                                ], axis=2)\n",
    "#         x = tf.concat([x_history, x_product, x_user], axis=2)\n",
    "#         return x    \n",
    "#     def call(self, inputs):\n",
    "#         for placeholder_name, data in inputs:\n",
    "#             if hasattr(self, placeholder_name):\n",
    "#                 setattr(self, placeholder_name, np.asarray(data, dtype=np.int32))\n",
    "#         x = self.get_sequence()\n",
    "#         h = self.lstm(x)\n",
    "#         h = tf.concat([h, x], axis=2)\n",
    "#         h_final = self.tddl1(h)\n",
    "#         params = self.tddl2(h_final)\n",
    "#         ps, mixing_coefs = tf.split(params, 2, axis=2)\n",
    "#         mixing_coefs = tf.keras.activations.softmax(mixing_coefs - tf.reduce_min(mixing_coefs, 2, keepdims=True))\n",
    "#         ps = tf.keras.activations.sigmoid(ps)\n",
    "#         labels = tf.tile(tf.expand_dims(self.next_is_ordered, 2), (1, 1, 1))\n",
    "#         losses = tf.reduce_sum(mixing_coefs*log_loss(labels, ps), axis=2)\n",
    "#         sequence_mask = tf.cast(tf.sequence_mask(self.history_length, maxlen=100), tf.float32)\n",
    "#         avg_loss = tf.reduce_sum(losses*sequence_mask) / tf.cast(tf.reduce_sum(self.history_length), tf.float32)\n",
    "\n",
    "#         final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), self.history_length - 1], axis=1)\n",
    "#         self.final_states = tf.gather_nd(h_final, final_temporal_idx)\n",
    "\n",
    "#         self.prediction_tensors = {\n",
    "#             'user_ids': self.user_id,\n",
    "#             'product_ids': self.product_id,\n",
    "#             'final_states': self.final_states\n",
    "#         }\n",
    "\n",
    "#         return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:25:02.614417Z",
     "iopub.status.busy": "2022-06-15T13:25:02.613317Z",
     "iopub.status.idle": "2022-06-15T13:25:02.628713Z",
     "shell.execute_reply": "2022-06-15T13:25:02.627167Z",
     "shell.execute_reply.started": "2022-06-15T13:25:02.614364Z"
    }
   },
   "outputs": [],
   "source": [
    "def sequence_log_loss(y, y_hat, sequence_lengths, max_sequence_length, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates average log loss on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length, input units].\n",
    "        y_hat: Prediction tensor, same shape as y.\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "        max_sequence_length: maximum length of padded sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        Log loss. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.math.minimum(tf.math.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_losses = y*tf.math.log(y_hat) + (1.0 - y)*tf.math.log(1.0 - y_hat)\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
    "    avg_log_loss = -tf.reduce_sum(log_losses*sequence_mask) / tf.cast(tf.math.reduce_sum(sequence_lengths), tf.float32)\n",
    "    return avg_log_loss\n",
    "def log_loss(y, y_hat, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates log loss between two tensors.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor.\n",
    "        y_hat: Prediction tensor\n",
    "\n",
    "    Returns:\n",
    "        Log loss. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.math.minimum(tf.math.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_loss = -tf.math.reduce_mean(y*tf.math.log(y_hat) + (1.0 - y)*tf.math.log(1.0 - y_hat))\n",
    "    return log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data reader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:25:07.131913Z",
     "iopub.status.busy": "2022-06-15T13:25:07.131512Z",
     "iopub.status.idle": "2022-06-15T13:25:07.155769Z",
     "shell.execute_reply": "2022-06-15T13:25:07.154551Z",
     "shell.execute_reply.started": "2022-06-15T13:25:07.131883Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataFrame(object):\n",
    "\n",
    "    \"\"\"Minimal pd.DataFrame analog for handling n-dimensional numpy matrices with additional\n",
    "    support for shuffling, batching, and train/test splitting.\n",
    "\n",
    "    Args:\n",
    "        columns: List of names corresponding to the matrices in data.\n",
    "        data: List of n-dimensional data matrices ordered in correspondence with columns.\n",
    "            All matrices must have the same leading dimension.  Data can also be fed a list of\n",
    "            instances of np.memmap, in which case RAM usage can be limited to the size of a\n",
    "            single batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, columns, data):\n",
    "        assert len(columns) == len(data), 'columns length does not match data length'\n",
    "\n",
    "        lengths = [mat.shape[0] for mat in data]\n",
    "        assert len(set(lengths)) == 1, 'all matrices in data must have same first dimension'\n",
    "\n",
    "        self.length = lengths[0]\n",
    "        self.columns = columns\n",
    "        self.data = data\n",
    "        self.dict = dict(zip(self.columns, self.data))\n",
    "        self.idx = np.arange(self.length)\n",
    "\n",
    "    def shapes(self):\n",
    "        return pd.Series(dict(zip(self.columns, [mat.shape for mat in self.data])))\n",
    "\n",
    "    def dtypes(self):\n",
    "        return pd.Series(dict(zip(self.columns, [mat.dtype for mat in self.data])))\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.idx)\n",
    "\n",
    "    def train_test_split(self, train_size, random_state=np.random.randint(10000)):\n",
    "        train_idx, test_idx = train_test_split(self.idx, train_size=train_size, random_state=random_state)\n",
    "        train_df = DataFrame(copy.copy(self.columns), [mat[train_idx] for mat in self.data])\n",
    "        test_df = DataFrame(copy.copy(self.columns), [mat[test_idx] for mat in self.data])\n",
    "        return train_df, test_df\n",
    "\n",
    "    def batch_generator(self, batch_size, shuffle=True, num_epochs=3, allow_smaller_final_batch=False):\n",
    "        epoch_num = 0\n",
    "        while epoch_num < num_epochs:\n",
    "            if shuffle:\n",
    "                self.shuffle()\n",
    "\n",
    "            for i in range(0, self.length, batch_size):\n",
    "                batch_idx = self.idx[i: i + batch_size]\n",
    "                if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n",
    "                    break\n",
    "                yield DataFrame(columns=copy.copy(self.columns), data=[mat[batch_idx].copy() for mat in self.data])\n",
    "\n",
    "            epoch_num += 1\n",
    "\n",
    "    def iterrows(self):\n",
    "        for i in self.idx:\n",
    "            yield self[i]\n",
    "\n",
    "    def mask(self, mask):\n",
    "        return DataFrame(copy.copy(self.columns), [mat[mask] for mat in self.data])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dict.items().__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            return self.dict[key]\n",
    "\n",
    "        elif isinstance(key, int):\n",
    "            return pd.Series(dict(zip(self.columns, [mat[self.idx[key]] for mat in self.data])))\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        assert value.shape[0] == len(self), 'matrix first dimension does not match'\n",
    "        if key not in self.columns:\n",
    "            self.columns.append(key)\n",
    "            self.data.append(value)\n",
    "        self.dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:25:10.170135Z",
     "iopub.status.busy": "2022-06-15T13:25:10.169754Z",
     "iopub.status.idle": "2022-06-15T13:25:10.187525Z",
     "shell.execute_reply": "2022-06-15T13:25:10.186325Z",
     "shell.execute_reply.started": "2022-06-15T13:25:10.170105Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'user_id',\n",
    "            'product_id',\n",
    "            'aisle_id',\n",
    "            'department_id',\n",
    "            'is_ordered_history',\n",
    "            'index_in_order_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'order_size_history',\n",
    "            'reorder_size_history',\n",
    "            'order_number_history',\n",
    "            'history_length',\n",
    "            'product_name',\n",
    "            'product_name_length',\n",
    "            'eval_set',\n",
    "            'label'\n",
    "        ]\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9)\n",
    "\n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=50,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=50,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=3, is_test=False):\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        for batch in batch_gen:\n",
    "            # np.roll shift array\n",
    "            batch['order_dow_history'] = np.roll(batch['order_dow_history'], -1, axis=1)\n",
    "            batch['order_hour_history'] = np.roll(batch['order_hour_history'], -1, axis=1)\n",
    "            batch['days_since_prior_order_history'] = np.roll(batch['days_since_prior_order_history'], -1, axis=1)\n",
    "            batch['order_number_history'] = np.roll(batch['order_number_history'], -1, axis=1)\n",
    "            batch['next_is_ordered'] = np.roll(batch['is_ordered_history'], -1, axis=1)\n",
    "            batch['is_none'] = batch['product_id'] == 0\n",
    "            if not is_test:\n",
    "                batch['history_length'] = batch['history_length'] - 1\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:23:30.460995Z",
     "iopub.status.busy": "2022-06-14T07:23:30.460659Z",
     "iopub.status.idle": "2022-06-14T07:23:31.362725Z",
     "shell.execute_reply": "2022-06-14T07:23:31.361216Z",
     "shell.execute_reply.started": "2022-06-14T07:23:30.460965Z"
    }
   },
   "outputs": [],
   "source": [
    "dr = DataReader(data_dir=os.path.join(base_dir, 'instacart-product'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:23:31.364868Z",
     "iopub.status.busy": "2022-06-14T07:23:31.363929Z",
     "iopub.status.idle": "2022-06-14T07:23:31.368753Z",
     "shell.execute_reply": "2022-06-14T07:23:31.367698Z",
     "shell.execute_reply.started": "2022-06-14T07:23:31.364832Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = dr.train_batch_generator(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-14T07:23:31.370764Z",
     "iopub.status.busy": "2022-06-14T07:23:31.370432Z",
     "iopub.status.idle": "2022-06-14T07:23:31.395507Z",
     "shell.execute_reply": "2022-06-14T07:23:31.394608Z",
     "shell.execute_reply.started": "2022-06-14T07:23:31.370735Z"
    }
   },
   "outputs": [],
   "source": [
    "val_dataset = dr.val_batch_generator(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:25:16.509891Z",
     "iopub.status.busy": "2022-06-15T13:25:16.509453Z",
     "iopub.status.idle": "2022-06-15T13:25:17.558602Z",
     "shell.execute_reply": "2022-06-15T13:25:17.557645Z",
     "shell.execute_reply.started": "2022-06-15T13:25:16.509857Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn = rnn_product(lstm_size = 300, dilations = [2**i for i in range(6)], filter_widths = [2]*6, skip_channels=64, residual_channels=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epochs = 80\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "step0 = 0\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    train = list()\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        step0 += 1\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = rnn(x_batch_train, training = True)\n",
    "            loss_value = sequence_log_loss(rnn.next_is_ordered, preds, rnn.history_length, 100)\n",
    "            train.append(loss_value)\n",
    "        grads = tape.gradient(loss_value, rnn.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, rnn.trainable_weights))\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * 128))\n",
    "        if step % 2728 == 0 and step > 0:\n",
    "            print(sum(train)/len(train))\n",
    "            break\n",
    "        if step0 > 60000:\n",
    "            break\n",
    "    if step0 > 60000:\n",
    "        break\n",
    "    val = list()\n",
    "    for step1, x_batch_val in enumerate(val_dataset):\n",
    "        val_preds = rnn(x_batch_val, training=True)\n",
    "        loss_value = sequence_log_loss(rnn.next_is_ordered, val_preds, rnn.history_length, 100)\n",
    "        val.append(loss_value)\n",
    "        if step1 % 10 == 0:\n",
    "            print(\"validation\", step1)\n",
    "            print(loss_value)\n",
    "        if step1 % 303 == 0 and step1 > 0:\n",
    "            print(sum(val)/len(val))\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:25:28.042132Z",
     "iopub.status.busy": "2022-06-15T13:25:28.041613Z",
     "iopub.status.idle": "2022-06-15T13:25:28.066150Z",
     "shell.execute_reply": "2022-06-15T13:25:28.065153Z",
     "shell.execute_reply.started": "2022-06-15T13:25:28.042085Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataReader_test(object):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'user_id',\n",
    "            'product_id',\n",
    "            'aisle_id',\n",
    "            'department_id',\n",
    "            'is_ordered_history',\n",
    "            'index_in_order_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'order_size_history',\n",
    "            'reorder_size_history',\n",
    "            'order_number_history',\n",
    "            'history_length',\n",
    "            'product_name',\n",
    "            'product_name_length',\n",
    "            'eval_set',\n",
    "            'label'\n",
    "        ]\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9)\n",
    "\n",
    "    def train_batch_generator(self):\n",
    "        return self.batch_generator(\n",
    "            batch_size=128,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=5,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=3, is_test=False):\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        for batch in batch_gen:\n",
    "            # np.roll shift array\n",
    "            batch['order_dow_history'] = np.roll(batch['order_dow_history'], -1, axis=1)\n",
    "            batch['order_hour_history'] = np.roll(batch['order_hour_history'], -1, axis=1)\n",
    "            batch['days_since_prior_order_history'] = np.roll(batch['days_since_prior_order_history'], -1, axis=1)\n",
    "            batch['order_number_history'] = np.roll(batch['order_number_history'], -1, axis=1)\n",
    "            batch['next_is_ordered'] = np.roll(batch['is_ordered_history'], -1, axis=1)\n",
    "            batch['is_none'] = batch['product_id'] == 0\n",
    "            if not is_test:\n",
    "                batch['history_length'] = batch['history_length'] - 1\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature extractor for test file for prediction (similar for other models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:25:31.332574Z",
     "iopub.status.busy": "2022-06-15T13:25:31.332015Z",
     "iopub.status.idle": "2022-06-15T13:25:32.134358Z",
     "shell.execute_reply": "2022-06-15T13:25:32.133447Z",
     "shell.execute_reply.started": "2022-06-15T13:25:31.332527Z"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '../input/'\n",
    "\n",
    "dr_test = DataReader_test(data_dir=os.path.join(base_dir, 'instacart-product'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:25:34.403687Z",
     "iopub.status.busy": "2022-06-15T13:25:34.402501Z",
     "iopub.status.idle": "2022-06-15T13:25:34.408110Z",
     "shell.execute_reply": "2022-06-15T13:25:34.407027Z",
     "shell.execute_reply.started": "2022-06-15T13:25:34.403647Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = dr_test.train_batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:32:24.817028Z",
     "iopub.status.busy": "2022-06-15T13:32:24.815645Z",
     "iopub.status.idle": "2022-06-15T13:32:24.823203Z",
     "shell.execute_reply": "2022-06-15T13:32:24.821828Z",
     "shell.execute_reply.started": "2022-06-15T13:32:24.816971Z"
    }
   },
   "outputs": [],
   "source": [
    "lst_user_ids = list()\n",
    "lst_product_ids = list()\n",
    "lst_final_states = list()\n",
    "lst_predictions = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T13:32:26.298805Z",
     "iopub.status.busy": "2022-06-15T13:32:26.298098Z",
     "iopub.status.idle": "2022-06-15T13:47:24.458248Z",
     "shell.execute_reply": "2022-06-15T13:47:24.456370Z",
     "shell.execute_reply.started": "2022-06-15T13:32:26.298750Z"
    }
   },
   "outputs": [],
   "source": [
    "for step, x_batch_test in enumerate(test_dataset):\n",
    "    preds = rnn(x_batch_test)\n",
    "    lst_user_ids = lst_user_ids + list(rnn.prediction_tensors[\"user_ids\"])\n",
    "    lst_product_ids = lst_product_ids +  list(rnn.prediction_tensors[\"product_ids\"])\n",
    "    lst_final_states = lst_final_states + list(rnn.prediction_tensors[\"final_states\"].numpy())\n",
    "    lst_predictions = lst_predictions + list(rnn.prediction_tensors[\"predictions\"].numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
