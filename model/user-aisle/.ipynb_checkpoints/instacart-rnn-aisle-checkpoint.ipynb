{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-15T11:40:27.880922Z",
     "iopub.status.busy": "2022-06-15T11:40:27.880409Z",
     "iopub.status.idle": "2022-06-15T11:40:32.570867Z",
     "shell.execute_reply": "2022-06-15T11:40:32.569992Z",
     "shell.execute_reply.started": "2022-06-15T11:40:27.880798Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T11:40:32.575001Z",
     "iopub.status.busy": "2022-06-15T11:40:32.574504Z",
     "iopub.status.idle": "2022-06-15T11:40:32.589604Z",
     "shell.execute_reply": "2022-06-15T11:40:32.588378Z",
     "shell.execute_reply.started": "2022-06-15T11:40:32.574974Z"
    }
   },
   "outputs": [],
   "source": [
    "def sequence_log_loss(y, y_hat, sequence_lengths, max_sequence_length, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates average log loss on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length, input units].\n",
    "        y_hat: Prediction tensor, same shape as y.\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "        max_sequence_length: maximum length of padded sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        Log loss. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.math.minimum(tf.math.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_losses = y*tf.math.log(y_hat) + (1.0 - y)*tf.math.log(1.0 - y_hat)\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
    "    avg_log_loss = -tf.reduce_sum(log_losses*sequence_mask) / tf.cast(tf.math.reduce_sum(sequence_lengths), tf.float32)\n",
    "    return avg_log_loss\n",
    "def log_loss(y, y_hat, eps=1e-15):\n",
    "    \"\"\"\n",
    "    Calculates log loss between two tensors.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor.\n",
    "        y_hat: Prediction tensor\n",
    "\n",
    "    Returns:\n",
    "        Log loss. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.math.minimum(tf.math.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_loss = -tf.math.reduce_mean(y*tf.math.log(y_hat) + (1.0 - y)*tf.math.log(1.0 - y_hat))\n",
    "    return log_loss\n",
    "def sequence_rmse(y, y_hat, sequence_lengths, max_sequence_length):\n",
    "    \"\"\"\n",
    "    Calculates RMSE on variable length sequences.\n",
    "\n",
    "    Args:\n",
    "        y: Label tensor of shape [batch size, max_sequence_length, input units].\n",
    "        y_hat: Prediction tensor, same shape as y.\n",
    "        sequence_lengths: Sequence lengths.  Tensor of shape [batch_size].\n",
    "        max_sequence_length: maximum length of padded sequence tensor.\n",
    "\n",
    "    Returns:\n",
    "        RMSE. 0-dimensional tensor.\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    squared_error = tf.square(y - y_hat)\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
    "    avg_squared_error = tf.math.reduce_sum(squared_error*sequence_mask) / tf.cast(tf.math.reduce_sum(sequence_lengths), tf.float32)\n",
    "    rmse = tf.math.sqrt(avg_squared_error)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T11:40:32.592039Z",
     "iopub.status.busy": "2022-06-15T11:40:32.591418Z",
     "iopub.status.idle": "2022-06-15T11:40:32.623475Z",
     "shell.execute_reply": "2022-06-15T11:40:32.622689Z",
     "shell.execute_reply.started": "2022-06-15T11:40:32.591998Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataFrame(object):\n",
    "\n",
    "    \"\"\"Minimal pd.DataFrame analog for handling n-dimensional numpy matrices with additional\n",
    "    support for shuffling, batching, and train/test splitting.\n",
    "\n",
    "    Args:\n",
    "        columns: List of names corresponding to the matrices in data.\n",
    "        data: List of n-dimensional data matrices ordered in correspondence with columns.\n",
    "            All matrices must have the same leading dimension.  Data can also be fed a list of\n",
    "            instances of np.memmap, in which case RAM usage can be limited to the size of a\n",
    "            single batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, columns, data):\n",
    "        assert len(columns) == len(data), 'columns length does not match data length'\n",
    "\n",
    "        lengths = [mat.shape[0] for mat in data]\n",
    "        assert len(set(lengths)) == 1, 'all matrices in data must have same first dimension'\n",
    "\n",
    "        self.length = lengths[0]\n",
    "        self.columns = columns\n",
    "        self.data = data\n",
    "        self.dict = dict(zip(self.columns, self.data))\n",
    "        self.idx = np.arange(self.length)\n",
    "\n",
    "    def shapes(self):\n",
    "        return pd.Series(dict(zip(self.columns, [mat.shape for mat in self.data])))\n",
    "\n",
    "    def dtypes(self):\n",
    "        return pd.Series(dict(zip(self.columns, [mat.dtype for mat in self.data])))\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.idx)\n",
    "\n",
    "    def train_test_split(self, train_size, random_state=np.random.randint(10000)):\n",
    "        train_idx, test_idx = train_test_split(self.idx, train_size=train_size, random_state=random_state)\n",
    "        train_df = DataFrame(copy.copy(self.columns), [mat[train_idx] for mat in self.data])\n",
    "        test_df = DataFrame(copy.copy(self.columns), [mat[test_idx] for mat in self.data])\n",
    "        return train_df, test_df\n",
    "\n",
    "    def batch_generator(self, batch_size, shuffle=True, num_epochs=3, allow_smaller_final_batch=False):\n",
    "        epoch_num = 0\n",
    "        while epoch_num < num_epochs:\n",
    "            if shuffle:\n",
    "                self.shuffle()\n",
    "\n",
    "            for i in range(0, self.length, batch_size):\n",
    "                batch_idx = self.idx[i: i + batch_size]\n",
    "                if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n",
    "                    break\n",
    "                yield DataFrame(columns=copy.copy(self.columns), data=[mat[batch_idx].copy() for mat in self.data])\n",
    "\n",
    "            epoch_num += 1\n",
    "\n",
    "    def iterrows(self):\n",
    "        for i in self.idx:\n",
    "            yield self[i]\n",
    "\n",
    "    def mask(self, mask):\n",
    "        return DataFrame(copy.copy(self.columns), [mat[mask] for mat in self.data])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dict.items().__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            return self.dict[key]\n",
    "\n",
    "        elif isinstance(key, int):\n",
    "            return pd.Series(dict(zip(self.columns, [mat[self.idx[key]] for mat in self.data])))\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        assert value.shape[0] == len(self), 'matrix first dimension does not match'\n",
    "        if key not in self.columns:\n",
    "            self.columns.append(key)\n",
    "            self.data.append(value)\n",
    "        self.dict[key] = value\n",
    "class DataReader(object):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        data_cols = [\n",
    "            'user_id',\n",
    "            'aisle_id',\n",
    "            'department_id',\n",
    "            'eval_set',\n",
    "            'is_ordered_history',\n",
    "            'index_in_order_history',\n",
    "            'order_dow_history',\n",
    "            'order_hour_history',\n",
    "            'days_since_prior_order_history',\n",
    "            'order_size_history',\n",
    "            'order_number_history',\n",
    "            'num_products_from_aisle_history',\n",
    "            'history_length',\n",
    "        ]\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "        self.test_df = DataFrame(columns=data_cols, data=data)\n",
    "\n",
    "\n",
    "        self.train_df, self.val_df = self.test_df.train_test_split(train_size=0.9)\n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=80,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=80,\n",
    "            is_test=False\n",
    "        )\n",
    "\n",
    "    def test_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.test_df,\n",
    "            shuffle=False,\n",
    "            num_epochs=1,\n",
    "            is_test=True\n",
    "        )\n",
    "\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        batch_gen = df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)\n",
    "        for batch in batch_gen:\n",
    "            batch['order_dow_history'] = np.roll(batch['order_dow_history'], -1, axis=1)\n",
    "            batch['order_hour_history'] = np.roll(batch['order_hour_history'], -1, axis=1)\n",
    "            batch['days_since_prior_order_history'] = np.roll(batch['days_since_prior_order_history'], -1, axis=1)\n",
    "            batch['order_number_history'] = np.roll(batch['order_number_history'], -1, axis=1)\n",
    "            batch['next_is_ordered'] = np.roll(batch['is_ordered_history'], -1, axis=1)\n",
    "            if not is_test:\n",
    "                batch['history_length'] = batch['history_length'] - 1\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T11:40:32.628124Z",
     "iopub.status.busy": "2022-06-15T11:40:32.627794Z",
     "iopub.status.idle": "2022-06-15T11:40:33.672362Z",
     "shell.execute_reply": "2022-06-15T11:40:33.671492Z",
     "shell.execute_reply.started": "2022-06-15T11:40:32.628099Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, state_size, return_final_state=False):\n",
    "        super().__init__()\n",
    "        self.cell_fw = tf.keras.layers.LSTMCell(state_size)\n",
    "        self.return_final_state = return_final_state\n",
    "        self.rnn_layers = tf.keras.layers.RNN(cell = [tf.keras.layers.LSTMCell(state_size)], return_sequences=True, return_state=True)\n",
    "    def call(self, inputs):\n",
    "        outputs, output_state = self.rnn_layers(inputs)\n",
    "        if self.return_final_state:\n",
    "            return outputs, output_state\n",
    "        else:\n",
    "            return outputs\n",
    "class TCL(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_units, convolution_width, causal=False, dilation_rate=[32], bias=True, activation=None, dropout=None):\n",
    "        super().__init__()\n",
    "        self.output_units = output_units\n",
    "        self.convolution_width = convolution_width\n",
    "        self.causal = causal\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "    def build(self, inputs_shape):\n",
    "        self.w = tf.Variable(name='weights', initial_value=tf.keras.initializers.VarianceScaling()(shape=[self.convolution_width, inputs_shape[2], self.output_units]), dtype=tf.float32)\n",
    "        if self.bias:\n",
    "            self.b = tf.Variable(name='biases', initial_value=tf.constant_initializer(0.1)(shape=[self.output_units]), dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        if self.causal:\n",
    "            shift = self.dilation_rate[0]*(self.convolution_width-1)\n",
    "            pad = tf.zeros(shape=[tf.shape(inputs)[0], shift, inputs.shape.as_list()[2]])\n",
    "            inputs = tf.concat([pad, inputs], axis=1)\n",
    "        z = tf.nn.convolution(inputs, self.w, padding='VALID', dilations=self.dilation_rate)\n",
    "        if self.bias:\n",
    "            z = z+self.b\n",
    "        z = self.activation(z) if self.activation else z\n",
    "        dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
    "        z = dropout_layer(z) if self.dropout is not None else z\n",
    "        return z\n",
    "class Time_distributed_dense_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self,output_units, bias=True, activation=None, batch_norm=None, dropout=None):\n",
    "        super().__init__()\n",
    "        self.output_units = output_units\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "    def build(self, inputs_shape):\n",
    "        self.w = tf.Variable(name='weights', initial_value=tf.keras.initializers.VarianceScaling()(shape=[inputs_shape[-1], self.output_units]), dtype=tf.float32)\n",
    "        if self.bias:\n",
    "            self.b = tf.Variable(name='biases', initial_value=tf.constant_initializer(0.1)(shape=[self.output_units]), dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        z = tf.einsum('ijk,kl->ijl', inputs, self.w)\n",
    "        if self.bias:\n",
    "            z = z + self.b\n",
    "        if self.batch_norm is not None:\n",
    "            batch_layer = tf.keras.layers.BatchNormalization()\n",
    "            z = batch_layer(z, training=self.batch_norm)\n",
    "        z = self.activation(z) if self.activation else z\n",
    "        dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
    "        z = dropout_layer(z) if self.dropout is not None else z\n",
    "        return z\n",
    "\n",
    "class Dense_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_units, bias=True, activation=None, batch_norm=None, dropout=None):\n",
    "        super().__init__()\n",
    "        self.output_units = output_units\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.batch_norm = batch_norm\n",
    "        self.dropout = dropout\n",
    "    def build(self, inputs_shape):\n",
    "        self.w = tf.Variable(name='weights', initial_value=tf.keras.initializers.VarianceScaling()(shape=[inputs_shape[-1], self.output_units]), dtype=tf.float32)\n",
    "        if self.bias:\n",
    "            self.b = tf.Variable(name='biases', initial_value=tf.constant_initializer(0.05)(shape=[self.output_units]), dtype=tf.float32)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.w)\n",
    "        if self.bias:\n",
    "            z = z+self.b\n",
    "        if self.batch_norm is not None:\n",
    "            batch_layer = tf.keras.layers.BatchNormalization()\n",
    "            z = batch_layer(z, training=self.batch_norm)\n",
    "        z = self.activation(z) if self.activation else z\n",
    "        dropout_layer = tf.keras.layers.Dropout(self.dropout)\n",
    "        z = dropout_layer(z) if self.dropout is not None else z\n",
    "        return z\n",
    "\n",
    "class Wavenet(tf.keras.layers.Layer):\n",
    "    def __init__(self, dilations, filter_widths, skip_channels, residual_channels):\n",
    "        super().__init__()\n",
    "        self.dilations = dilations\n",
    "        self.filter_widths = filter_widths\n",
    "        self.skip_channels = skip_channels\n",
    "        self.residual_channels = residual_channels\n",
    "        self.tddl0 = Time_distributed_dense_layer(output_units = residual_channels, activation=tf.keras.activations.tanh)\n",
    "        self.tcl1 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[0]])\n",
    "        self.tcl2 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[1]])\n",
    "        self.tcl3 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[2]])\n",
    "        self.tcl4 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[3]])\n",
    "        self.tcl5 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[4]])\n",
    "        self.tcl6 = TCL(output_units = 2*residual_channels, convolution_width = 2, causal=True, dilation_rate=[dilations[5]])\n",
    "        self.list_tcl = list()\n",
    "        self.list_tcl.append(self.tcl1)\n",
    "        self.list_tcl.append(self.tcl2)\n",
    "        self.list_tcl.append(self.tcl3)\n",
    "        self.list_tcl.append(self.tcl4)\n",
    "        self.list_tcl.append(self.tcl5)\n",
    "        self.list_tcl.append(self.tcl6)\n",
    "        self.tddl1 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.tddl2 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.tddl3 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.tddl4 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.tddl5 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.tddl6 = Time_distributed_dense_layer(output_units = residual_channels + skip_channels)\n",
    "        self.list_tddl = list()\n",
    "        self.list_tddl.append(self.tddl1)\n",
    "        self.list_tddl.append(self.tddl2)\n",
    "        self.list_tddl.append(self.tddl3)\n",
    "        self.list_tddl.append(self.tddl4)\n",
    "        self.list_tddl.append(self.tddl5)\n",
    "        self.list_tddl.append(self.tddl6)\n",
    "    def call(self, inputs):\n",
    "        z = self.tddl0(inputs)\n",
    "        skip_outputs = []\n",
    "        for i in range(6):\n",
    "            dilated_conv = self.list_tcl[i](z)\n",
    "            conv_filter, conv_gate = tf.split(dilated_conv, 2, axis=2)\n",
    "            dilated_conv = tf.keras.activations.tanh(conv_filter)*tf.keras.activations.sigmoid(conv_gate)\n",
    "            output_units = self.skip_channels + self.residual_channels\n",
    "            outputs = self.list_tddl[i](dilated_conv)\n",
    "            skips, residuals = tf.split(outputs, [self.skip_channels, self.residual_channels], axis=2)\n",
    "            z += residuals\n",
    "            skip_outputs.append(skips)\n",
    "        skip_outputs = tf.keras.activations.relu(tf.concat(skip_outputs, axis=2))\n",
    "        return skip_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T11:40:33.67434Z",
     "iopub.status.busy": "2022-06-15T11:40:33.673717Z",
     "iopub.status.idle": "2022-06-15T11:40:33.703361Z",
     "shell.execute_reply": "2022-06-15T11:40:33.702721Z",
     "shell.execute_reply.started": "2022-06-15T11:40:33.6743Z"
    }
   },
   "outputs": [],
   "source": [
    "class rnn_aisle(tf.keras.Model):\n",
    "    def __init__(self, lstm_size):\n",
    "        super().__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.user_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.aisle_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.department_id = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.history_length = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.is_ordered_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.index_in_order_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.order_dow_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.order_hour_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.days_since_prior_order_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.order_size_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.order_number_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.num_products_from_aisle_history = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.next_is_ordered = tf.keras.Input(shape=[100], dtype = tf.int32)\n",
    "        self.keep_prob = tf.keras.Input(shape=[], dtype = tf.float32)\n",
    "        self.is_training = tf.keras.Input(shape=[], dtype = tf.bool)\n",
    "        self.lstm = LSTM_layer(lstm_size)\n",
    "        self.tddl1 = Time_distributed_dense_layer(50, activation=tf.keras.activations.relu)\n",
    "        self.tddl2 = Time_distributed_dense_layer(1, activation=tf.keras.activations.sigmoid)\n",
    "        \n",
    "        self.aisle_embeddings = tf.Variable(name='aisle_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[250, 50]), dtype=tf.float32)\n",
    "        self.department_embeddings = tf.Variable(name='department_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[50, 10]), dtype=tf.float32)\n",
    "        self.user_embeddings = tf.Variable(name='user_embeddings', initial_value=tf.keras.initializers.VarianceScaling()(shape=[207000, self.lstm_size]), dtype=tf.float32)\n",
    "        \n",
    "    def get_sequence(self):\n",
    "        x_aisle = tf.concat([\n",
    "                               tf.nn.embedding_lookup(self.aisle_embeddings, self.aisle_id),\n",
    "                               tf.nn.embedding_lookup(self.department_embeddings, self.department_id),\n",
    "                               ], axis=1)\n",
    "        x_aisle = tf.tile(tf.expand_dims(x_aisle, 1), (1, 100, 1))\n",
    "        x_user = tf.nn.embedding_lookup(self.user_embeddings, self.user_id)\n",
    "        x_user = tf.tile(tf.expand_dims(x_user, 1), (1, 100, 1))\n",
    "        is_ordered_history = tf.one_hot(self.is_ordered_history, 2)\n",
    "        index_in_order_history = tf.one_hot(self.index_in_order_history, 20)\n",
    "        order_dow_history = tf.one_hot(self.order_dow_history, 8)\n",
    "        order_hour_history = tf.one_hot(self.order_hour_history, 25)\n",
    "        days_since_prior_order_history = tf.one_hot(self.days_since_prior_order_history, 31)\n",
    "        order_size_history = tf.one_hot(self.order_size_history, 60)\n",
    "        index_in_order_history_scalar = tf.expand_dims(tf.cast(self.index_in_order_history, tf.float32) / 20.0, 2)\n",
    "        order_number_history = tf.one_hot(self.order_number_history, 101)\n",
    "        num_products_from_aisle_history = tf.one_hot(self.num_products_from_aisle_history, 50)\n",
    "        order_dow_history_scalar = tf.expand_dims(tf.cast(self.order_dow_history, tf.float32) / 8.0, 2)\n",
    "        order_hour_history_scalar = tf.expand_dims(tf.cast(self.order_hour_history, tf.float32) / 25.0, 2)\n",
    "        days_since_prior_order_history_scalar = tf.expand_dims(tf.cast(self.days_since_prior_order_history, tf.float32) / 31.0, 2)\n",
    "        order_size_history_scalar = tf.expand_dims(tf.cast(self.order_size_history, tf.float32) / 60.0, 2)\n",
    "        order_number_history_scalar = tf.expand_dims(tf.cast(self.order_number_history, tf.float32) / 100.0, 2)\n",
    "        num_products_from_aisle_history_scalar = tf.expand_dims(tf.cast(self.num_products_from_aisle_history, tf.float32) / 50.0, 2)\n",
    "        x_history = tf.concat([\n",
    "            is_ordered_history,\n",
    "            index_in_order_history,\n",
    "            order_dow_history,\n",
    "            order_hour_history,\n",
    "            days_since_prior_order_history,\n",
    "            order_size_history,\n",
    "            num_products_from_aisle_history,\n",
    "            order_number_history,\n",
    "            index_in_order_history_scalar,\n",
    "            order_dow_history_scalar,\n",
    "            order_hour_history_scalar,\n",
    "            days_since_prior_order_history_scalar,\n",
    "            order_size_history_scalar,\n",
    "            order_number_history_scalar,\n",
    "            num_products_from_aisle_history_scalar,\n",
    "        ], axis=2)\n",
    "        x = tf.concat([x_history, x_aisle, x_user], axis=2)\n",
    "        return x    \n",
    "    def call(self, inputs):\n",
    "        for placeholder_name, data in inputs:\n",
    "            if hasattr(self, placeholder_name):\n",
    "                setattr(self, placeholder_name, np.asarray(data, dtype=np.int32))\n",
    "        x = self.get_sequence()\n",
    "        h = self.lstm(x)\n",
    "        h = tf.concat([h, x], axis=2)\n",
    "        self.h_final = self.tddl1(h)\n",
    "        y_hat = self.tddl2(self.h_final)\n",
    "        y_hat = tf.squeeze(y_hat, 2)\n",
    "        final_temporal_idx = tf.stack([tf.range(tf.shape(self.history_length)[0]), self.history_length - 1], axis=1)\n",
    "        self.final_states = tf.gather_nd(self.h_final, final_temporal_idx)\n",
    "        self.final_predictions = tf.gather_nd(y_hat, final_temporal_idx)\n",
    "        self.prediction_tensors = {\n",
    "            'user_ids': self.user_id,\n",
    "            'aisle_ids': self.aisle_id,\n",
    "            'final_states': self.final_states,\n",
    "            'predictions': self.final_predictions\n",
    "        }\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T11:40:33.704608Z",
     "iopub.status.busy": "2022-06-15T11:40:33.704362Z",
     "iopub.status.idle": "2022-06-15T11:40:35.997384Z",
     "shell.execute_reply": "2022-06-15T11:40:35.996425Z",
     "shell.execute_reply.started": "2022-06-15T11:40:33.704585Z"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '../input/'\n",
    "dr = DataReader(data_dir=os.path.join(base_dir, 'instacartaisle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T11:40:35.999015Z",
     "iopub.status.busy": "2022-06-15T11:40:35.998646Z",
     "iopub.status.idle": "2022-06-15T11:40:38.54404Z",
     "shell.execute_reply": "2022-06-15T11:40:38.543226Z",
     "shell.execute_reply.started": "2022-06-15T11:40:35.998982Z"
    }
   },
   "outputs": [],
   "source": [
    "rnn = rnn_aisle(lstm_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T11:40:38.545787Z",
     "iopub.status.busy": "2022-06-15T11:40:38.545379Z",
     "iopub.status.idle": "2022-06-15T11:40:38.550359Z",
     "shell.execute_reply": "2022-06-15T11:40:38.549658Z",
     "shell.execute_reply.started": "2022-06-15T11:40:38.545734Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = dr.train_batch_generator(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T11:40:38.552364Z",
     "iopub.status.busy": "2022-06-15T11:40:38.55153Z",
     "iopub.status.idle": "2022-06-15T11:40:38.559147Z",
     "shell.execute_reply": "2022-06-15T11:40:38.558119Z",
     "shell.execute_reply.started": "2022-06-15T11:40:38.552324Z"
    }
   },
   "outputs": [],
   "source": [
    "val_dataset = dr.val_batch_generator(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T11:40:38.562646Z",
     "iopub.status.busy": "2022-06-15T11:40:38.56211Z",
     "iopub.status.idle": "2022-06-15T11:41:35.685596Z",
     "shell.execute_reply": "2022-06-15T11:41:35.684055Z",
     "shell.execute_reply.started": "2022-06-15T11:40:38.562605Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "epochs = 80\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "step0 = 0\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    train = list()\n",
    "    pred = list()\n",
    "    label = list()\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = rnn(x_batch_train, training = True)\n",
    "            pred.append(preds)\n",
    "            label.append(rnn.next_is_ordered)\n",
    "            loss_value = sequence_log_loss(rnn.next_is_ordered, preds, rnn.history_length, 100)\n",
    "            train.append(loss_value)\n",
    "        grads = tape.gradient(loss_value, rnn.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, rnn.trainable_weights))\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * 128))\n",
    "        if step % 2728 == 0 and step > 0:\n",
    "            print(sum(train)/len(train))\n",
    "            break\n",
    "        if step0 > 60000:\n",
    "            break\n",
    "    if step0 > 60000:\n",
    "        break\n",
    "    val = list()\n",
    "    for step1, x_batch_val in enumerate(val_dataset):\n",
    "        val_preds = rnn(x_batch_val, training=True)\n",
    "        loss_value = sequence_log_loss(rnn.next_is_ordered, val_preds, rnn.history_length, 100)\n",
    "        val.append(loss_value)\n",
    "        if step1 % 10 == 0:\n",
    "            print(\"validation\", step1)\n",
    "            print(loss_value)\n",
    "        if step1 % 303 == 0 and step1 > 0:\n",
    "            print(sum(val)/len(val))\n",
    "            break\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
