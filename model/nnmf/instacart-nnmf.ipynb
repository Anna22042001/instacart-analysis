{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-06-15T14:03:12.002950Z",
     "iopub.status.busy": "2022-06-15T14:03:12.001998Z",
     "iopub.status.idle": "2022-06-15T14:03:18.828410Z",
     "shell.execute_reply": "2022-06-15T14:03:18.827470Z",
     "shell.execute_reply.started": "2022-06-15T14:03:12.002837Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T14:03:18.830863Z",
     "iopub.status.busy": "2022-06-15T14:03:18.829919Z",
     "iopub.status.idle": "2022-06-15T14:03:18.852736Z",
     "shell.execute_reply": "2022-06-15T14:03:18.851891Z",
     "shell.execute_reply.started": "2022-06-15T14:03:18.830829Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataFrame(object):\n",
    "\n",
    "\n",
    "    def __init__(self, columns, data):\n",
    "        assert len(columns) == len(data), 'columns length does not match data length'\n",
    "\n",
    "        lengths = [mat.shape[0] for mat in data]\n",
    "        assert len(set(lengths)) == 1, 'all matrices in data must have same first dimension'\n",
    "\n",
    "        self.length = lengths[0]\n",
    "        self.columns = columns\n",
    "        self.data = data\n",
    "        self.dict = dict(zip(self.columns, self.data))\n",
    "        self.idx = np.arange(self.length)\n",
    "\n",
    "    def shapes(self):\n",
    "        return pd.Series(dict(zip(self.columns, [mat.shape for mat in self.data])))\n",
    "\n",
    "    def dtypes(self):\n",
    "        return pd.Series(dict(zip(self.columns, [mat.dtype for mat in self.data])))\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.idx)\n",
    "\n",
    "    def train_test_split(self, train_size, random_state=np.random.randint(10000)):\n",
    "        train_idx, test_idx = train_test_split(self.idx, train_size=train_size, random_state=random_state)\n",
    "        train_df = DataFrame(copy.copy(self.columns), [mat[train_idx] for mat in self.data])\n",
    "        test_df = DataFrame(copy.copy(self.columns), [mat[test_idx] for mat in self.data])\n",
    "        return train_df, test_df\n",
    "\n",
    "    def batch_generator(self, batch_size, shuffle=True, num_epochs=3, allow_smaller_final_batch=False):\n",
    "        epoch_num = 0\n",
    "        while epoch_num < num_epochs:\n",
    "            if shuffle:\n",
    "                self.shuffle()\n",
    "\n",
    "            for i in range(0, self.length, batch_size):\n",
    "                batch_idx = self.idx[i: i + batch_size]\n",
    "                if not allow_smaller_final_batch and len(batch_idx) != batch_size:\n",
    "                    break\n",
    "                yield DataFrame(columns=copy.copy(self.columns), data=[mat[batch_idx].copy() for mat in self.data])\n",
    "\n",
    "            epoch_num += 1\n",
    "\n",
    "    def iterrows(self):\n",
    "        for i in self.idx:\n",
    "            yield self[i]\n",
    "\n",
    "    def mask(self, mask):\n",
    "        return DataFrame(copy.copy(self.columns), [mat[mask] for mat in self.data])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.dict.items().__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, str):\n",
    "            return self.dict[key]\n",
    "\n",
    "        elif isinstance(key, int):\n",
    "            return pd.Series(dict(zip(self.columns, [mat[self.idx[key]] for mat in self.data])))\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        assert value.shape[0] == len(self), 'matrix first dimension does not match'\n",
    "        if key not in self.columns:\n",
    "            self.columns.append(key)\n",
    "            self.data.append(value)\n",
    "        self.dict[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T14:03:18.854419Z",
     "iopub.status.busy": "2022-06-15T14:03:18.853713Z",
     "iopub.status.idle": "2022-06-15T14:03:18.871171Z",
     "shell.execute_reply": "2022-06-15T14:03:18.870318Z",
     "shell.execute_reply.started": "2022-06-15T14:03:18.854387Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataReader(object):\n",
    "\n",
    "    def __init__(self, data_dir=\"../input/instacart-nnmf-dataset\"):\n",
    "        data_cols = ['i', 'j', 'V_ij']\n",
    "        data = [np.load(os.path.join(data_dir, '{}.npy'.format(i)), mmap_mode='r') for i in data_cols]\n",
    "\n",
    "        df = DataFrame(columns=data_cols, data=data)\n",
    "        self.train_df, self.val_df = df.train_test_split(train_size=0.9)\n",
    "\n",
    "        self.num_users = df['i'].max() + 1\n",
    "        self.num_products = df['j'].max() + 1\n",
    "        # num_users == num_products\n",
    "\n",
    "    def train_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.train_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=50\n",
    "        )\n",
    "\n",
    "    def val_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(\n",
    "            batch_size=batch_size,\n",
    "            df=self.val_df,\n",
    "            shuffle=True,\n",
    "            num_epochs=50\n",
    "        )\n",
    "\n",
    "    def batch_generator(self, batch_size, df, shuffle=True, num_epochs=10000, is_test=False):\n",
    "        return df.batch_generator(batch_size, shuffle=shuffle, num_epochs=num_epochs, allow_smaller_final_batch=is_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T14:03:18.873447Z",
     "iopub.status.busy": "2022-06-15T14:03:18.872918Z",
     "iopub.status.idle": "2022-06-15T14:03:18.887307Z",
     "shell.execute_reply": "2022-06-15T14:03:18.886509Z",
     "shell.execute_reply.started": "2022-06-15T14:03:18.873413Z"
    }
   },
   "outputs": [],
   "source": [
    "def sequence_log_loss(y, y_hat, sequence_lengths, max_sequence_length, eps=1e-15):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.math.minimum(tf.math.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_losses = y*tf.math.log(y_hat) + (1.0 - y)*tf.math.log(1.0 - y_hat)\n",
    "    sequence_mask = tf.cast(tf.sequence_mask(sequence_lengths, maxlen=max_sequence_length), tf.float32)\n",
    "    avg_log_loss = -tf.reduce_sum(log_losses*sequence_mask) / tf.cast(tf.math.reduce_sum(sequence_lengths), tf.float32)\n",
    "    return avg_log_loss\n",
    "def log_loss(y, y_hat, eps=1e-15):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.math.minimum(tf.math.maximum(y_hat, eps), 1.0 - eps)\n",
    "    log_loss = -tf.math.reduce_mean(y*tf.math.log(y_hat) + (1.0 - y)*tf.math.log(1.0 - y_hat))\n",
    "    return log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T14:03:18.889675Z",
     "iopub.status.busy": "2022-06-15T14:03:18.888945Z",
     "iopub.status.idle": "2022-06-15T14:03:20.302204Z",
     "shell.execute_reply": "2022-06-15T14:03:20.301113Z",
     "shell.execute_reply.started": "2022-06-15T14:03:18.889638Z"
    }
   },
   "outputs": [],
   "source": [
    "class NNMF (tf.keras.Model):\n",
    "    def __init__(self,rank, num_products, num_users):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.i = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.j = tf.keras.Input(shape=[], dtype = tf.int32)\n",
    "        self.V_ij = tf.keras.Input(shape=[], dtype = tf.float32)\n",
    "        self.num_users =num_users\n",
    "        self.num_products = num_products\n",
    "        self.W = tf.Variable(tf.random.truncated_normal([num_users, rank]))\n",
    "        self.H = tf.Variable(tf.random.truncated_normal([num_products, rank]))\n",
    "        self.W_bias = tf.Variable(tf.random.truncated_normal([num_users]))\n",
    "        self.H_bias = tf.Variable(tf.random.truncated_normal([num_products]))\n",
    "        self.global_mean = tf.Variable(0.0)\n",
    "    def call(self,inputs):\n",
    "        for placeholder_name, data in inputs:\n",
    "            if placeholder_name != \"V_ij\":\n",
    "                if hasattr(self, placeholder_name):\n",
    "                    setattr(self, placeholder_name, np.asarray(data, dtype=np.int32))\n",
    "            else:\n",
    "                if hasattr(self, placeholder_name):\n",
    "                    setattr(self, placeholder_name, np.asarray(data, dtype=np.float32))\n",
    "        w_i = tf.gather(self.W, self.i)\n",
    "        h_j = tf.gather(self.H, self.j)\n",
    "        w_bias = tf.gather(self.W_bias, self.i)\n",
    "        h_bias = tf.gather(self.H_bias, self.j)\n",
    "        interaction = tf.reduce_sum(w_i * h_j, axis=1)\n",
    "        preds = self.global_mean + w_bias + h_bias + interaction\n",
    "        rmse = tf.math.sqrt(tf.math.reduce_mean(tf.math.squared_difference(preds, self.V_ij)))\n",
    "        self.parameter_tensors = {\n",
    "            'user_embeddings': self.W,\n",
    "            'product_embeddings': self.H\n",
    "        }\n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T14:03:20.304531Z",
     "iopub.status.busy": "2022-06-15T14:03:20.304091Z",
     "iopub.status.idle": "2022-06-15T14:03:34.995967Z",
     "shell.execute_reply": "2022-06-15T14:03:34.994843Z",
     "shell.execute_reply.started": "2022-06-15T14:03:20.304482Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = '../input/'\n",
    "dr = DataReader(data_dir=os.path.join(base_dir, 'instacart-nnmf-dataset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T14:03:34.997807Z",
     "iopub.status.busy": "2022-06-15T14:03:34.997374Z",
     "iopub.status.idle": "2022-06-15T14:03:35.190728Z",
     "shell.execute_reply": "2022-06-15T14:03:35.189939Z",
     "shell.execute_reply.started": "2022-06-15T14:03:34.997748Z"
    }
   },
   "outputs": [],
   "source": [
    "nnmf = NNMF(rank = 25, num_products = dr.num_products, num_users = dr.num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T14:03:35.192317Z",
     "iopub.status.busy": "2022-06-15T14:03:35.191958Z",
     "iopub.status.idle": "2022-06-15T14:03:35.196846Z",
     "shell.execute_reply": "2022-06-15T14:03:35.195940Z",
     "shell.execute_reply.started": "2022-06-15T14:03:35.192286Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = dr.train_batch_generator(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T14:03:35.198647Z",
     "iopub.status.busy": "2022-06-15T14:03:35.198303Z",
     "iopub.status.idle": "2022-06-15T14:03:35.211256Z",
     "shell.execute_reply": "2022-06-15T14:03:35.209715Z",
     "shell.execute_reply.started": "2022-06-15T14:03:35.198615Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-15T14:03:35.214548Z",
     "iopub.status.busy": "2022-06-15T14:03:35.214122Z",
     "iopub.status.idle": "2022-06-15T14:05:11.731300Z",
     "shell.execute_reply": "2022-06-15T14:05:11.729820Z",
     "shell.execute_reply.started": "2022-06-15T14:03:35.214513Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "epochs = 50\n",
    "optimizer = tf.keras.optimizers.Adam(0.002)\n",
    "step0 = 0\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    train = list()\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        step0 += 1\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = nnmf(x_batch_train, training = True)\n",
    "            train.append(loss_value)\n",
    "        grads = tape.gradient(loss_value, nnmf.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, nnmf.trainable_weights))\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * 128))\n",
    "        if step % 2728 == 0 and step > 0:\n",
    "            print(sum(train)/len(train))\n",
    "            break\n",
    "        if step0 > 60000:\n",
    "            break\n",
    "    if step0 > 60000:\n",
    "        break\n",
    "    val = list()\n",
    "    for step1, x_batch_val in enumerate(val_dataset):\n",
    "        loss_value = nnmf(x_batch_val, training=True)\n",
    "        val.append(loss_value)\n",
    "        if step1 % 10 == 0:\n",
    "            print(\"validation\", step1)\n",
    "            print(loss_value)\n",
    "        if step1 % 303 == 0 and step1 > 0:\n",
    "            print(sum(val)/len(val))\n",
    "            break\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
